# 🤖 Agent QA 테스트 시스템 - 발표용 워크플로우

## 📋 시스템 개요 (30초)
- **목적**: LLM 답변의 품질을 자동으로 평가하는 시스템
- **핵심**: Excel → LLM 답변 생성 → Judge 평가 → 결과 저장
- **특징**: 웹 UI + 자동화 + 히스토리 추적

---

## 🔄 핵심 워크플로우 (4단계)

### 1️⃣ 테스트케이스 준비
```
TestCase.xlsx 작성
├── case_id: TC_001, TC_002, ...
└── question: 평가할 질문들
```

### 2️⃣ 데이터 저장
```
Excel → LangSmith 데이터셋 저장
📊 Agent_QA_Scenario 데이터셋 생성
```

### 3️⃣ 자동 평가 실행
```
질문 → GPT-4o 답변 생성 → GPT-4o Judge 평가 (0-5점)
📈 실시간 결과 확인
```

### 4️⃣ 결과 분석
```
웹 UI에서 결과 확인
├── 평가 점수 및 근거
├── 히스토리 추적
└── 시각화 대시보드
```

---

## 🌐 웹 인터페이스 데모 순서

### 1. 메인 실행 탭
- **"TestCase.xlsx → LangSmith 저장"** 버튼 클릭
- **"GPT-4o 평가 실행/저장"** 버튼 클릭
- 실시간 로그 확인

### 2. 평가 결과 탭
- 평가 결과 테이블 확인
- 요약 통계 (평균/최고/최저 점수)
- 점수 분포 현황

### 3. 히스토리 조회 탭
- case_id별 점수 변화 그래프
- 상세 히스토리 테이블
- Trace 링크로 상세 분석

---

## 📊 평가 기준 (Judge 시스템)

| 점수 | 기준 |
|------|------|
| **5점** | 완벽히 정확하고 완전한 답변 |
| **4점** | 대부분 정확하며 약간의 부족함 |
| **3점** | 기본적으로 정확하나 중요 정보 누락 |
| **2점** | 부분적으로 정확하나 오류 포함 |
| **1점** | 대부분 부정확하나 일부 관련 정보 |
| **0점** | 완전히 부정확하거나 관련 없음 |

---

## 🎯 핵심 가치

### ✅ **자동화**
- 수동 평가 → 자동 평가로 효율성 극대화
- 일관된 평가 기준 적용

### ✅ **추적성**
- 모든 평가 결과 LangSmith에 저장
- 히스토리 누적으로 성능 변화 추적

### ✅ **시각화**
- 웹 UI로 직관적인 결과 확인
- 그래프와 통계로 인사이트 제공

### ✅ **확장성**
- 다양한 모델 지원 가능
- 커스텀 평가 기준 추가 가능

---

## 🚀 실행 방법 (1분)

```bash
# 1. 환경 설정 (.env 파일에 API 키 설정)
OPENAI_API_KEY=your_key
LANGCHAIN_API_KEY=your_key

# 2. 웹 인터페이스 실행
python new_project/web_interface.py

# 3. 브라우저에서 http://localhost:7861 접속
```

---

## 💡 데모 시나리오

### 시나리오: "AI 기초 지식 테스트"
1. **질문 예시**: "머신러닝과 딥러닝의 차이점은?"
2. **LLM 답변**: 자동 생성
3. **Judge 평가**: 4/5점 - "기본 개념은 정확하나 구체적 예시 부족"
4. **결과 저장**: LangSmith 데이터셋에 자동 저장
5. **히스토리**: 동일 질문의 이전 평가와 비교

---

## 🔧 기술 스택

- **LLM**: OpenAI GPT-4o
- **평가**: LLM-as-Judge 방식
- **저장소**: LangSmith 데이터셋
- **프롬프트 관리**: LangChain Hub
- **웹 UI**: Gradio
- **시각화**: Plotly

---

## 📈 기대 효과

### 🎯 **품질 관리**
- 객관적이고 일관된 평가 기준
- 대규모 테스트케이스 자동 처리

### 📊 **성능 추적**
- 시간별 성능 변화 모니터링
- 모델별 성능 비교 분석

### ⚡ **효율성**
- 수동 평가 시간 90% 단축
- 24/7 자동 평가 가능

---

**🎉 Demo Ready!**
