#!/usr/bin/env python3
"""
ì‹¤ì œ Agent QA ì‹œìŠ¤í…œ êµ¬í˜„
TestCase.xlsx ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ GPT-4oë¡œ ì§ˆì˜í•˜ê³  í‰ê°€í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import pandas as pd
import json
import time
from typing import Dict, List, Optional
import re
from pathlib import Path
from datetime import datetime

from langsmith import Client as LangSmithClient
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
import os
from langchain.callbacks.tracers.run_collector import RunCollectorCallbackHandler

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class RealAgentQASystem:
    """ì‹¤ì œ Agent QA ì‹œìŠ¤í…œ êµ¬í˜„"""
    
    def __init__(self):
        """ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.langsmith_client = LangSmithClient()
        
        # GPT-4o ëª¨ë¸ ì´ˆê¸°í™” (ì‹¤ì œ ì§ˆì˜ìš©)
        self.gpt_model = ChatOpenAI(
            model="gpt-4o",
            temperature=0.7  # ë‹µë³€ ë‹¤ì–‘ì„±ì„ ìœ„í•´ ì•½ê°„ì˜ temperature ì„¤ì •
        )
        
        # GPT-4o Judge ëª¨ë¸ ì´ˆê¸°í™” (í‰ê°€ìš©)
        self.judge_model = ChatOpenAI(
            model="gpt-4o",
            temperature=0  # í‰ê°€ì˜ ì¼ê´€ì„±ì„ ìœ„í•´ temperature 0
        )
        
        # Judge í”„ë¡¬í”„íŠ¸ ì„¤ì • - LangSmithì—ì„œ ê°€ì ¸ì˜¤ê¸°
        try:
            # LangSmithì—ì„œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹œë„
            self.accuracy_judge_prompt = self._load_prompt_from_langsmith("accuracy_judge_prompt")
            print("âœ… LangSmithì—ì„œ accuracy_judge_prompt ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸  LangSmithì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨, ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©: {e}")
            # í´ë°±: ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            self.accuracy_judge_prompt = ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ QA ì‹œìŠ¤í…œì˜ ë‹µë³€ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ Judgeì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì˜ ì •í™•ì„±ì„ 0-5ì  ì‚¬ì´ì˜ ì •ìˆ˜ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”.

í‰ê°€ ê¸°ì¤€:
- 5ì : ì™„ë²½íˆ ì •í™•í•˜ê³  ì™„ì „í•œ ë‹µë³€
- 4ì : ëŒ€ë¶€ë¶„ ì •í™•í•˜ë©° ì•½ê°„ì˜ ë¶€ì¡±í•¨ì´ ìˆìŒ
- 3ì : ê¸°ë³¸ì ìœ¼ë¡œ ì •í™•í•˜ë‚˜ ì¤‘ìš”í•œ ì •ë³´ê°€ ëˆ„ë½ë¨
- 2ì : ë¶€ë¶„ì ìœ¼ë¡œ ì •í™•í•˜ë‚˜ ì˜¤ë¥˜ë‚˜ ë¶€ì •í™•í•œ ì •ë³´ í¬í•¨
- 1ì : ëŒ€ë¶€ë¶„ ë¶€ì •í™•í•˜ë‚˜ ì¼ë¶€ ê´€ë ¨ëœ ì •ë³´ í¬í•¨
- 0ì : ì™„ì „íˆ ë¶€ì •í™•í•˜ê±°ë‚˜ ê´€ë ¨ ì—†ëŠ” ë‹µë³€

í‰ê°€ ê²°ê³¼ë¥¼ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:
{{"score": <0-5 ì‚¬ì´ì˜ ì •ìˆ˜>, "reasoning": "<í‰ê°€ ê·¼ê±°>"}}

ë°˜ë“œì‹œ ì •ìˆ˜ ì ìˆ˜ë§Œ ì‚¬ìš©í•˜ê³ , ì†Œìˆ˜ì ì€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”."""),
                ("human", "ì§ˆë¬¸: {question}\në‹µë³€: {answer}\n\nìœ„ ë‹µë³€ì„ í‰ê°€í•´ì£¼ì„¸ìš”.")
            ])
        
        self.judge_chain = self.accuracy_judge_prompt | self.judge_model | StrOutputParser()
        
        # OpenEvals ê´€ë ¨ì€ í˜„ì¬ ë¹„í™œì„±í™” (ë©”ë‰´ì—ì„œ ì œê±°ë¨)
        
        # ë°ì´í„°ì…‹ ì´ë¦„ ì„¤ì •
        self.source_dataset = "Agent_QA_Scenario"
        self.result_dataset = "Agent_QA_Scenario_Judge_Result"
        self.history_dataset = "Agent_QA_Scenario_Judge_History"
        self.base_web_url = self._get_base_web_url()
        # LangSmith tracing ê¸°ë³¸ê°’ ë³´ì¥
        if not os.getenv("LANGSMITH_PROJECT"):
            os.environ["LANGSMITH_PROJECT"] = "llm-practice"
        if not os.getenv("LANGSMITH_TRACING"):
            os.environ["LANGSMITH_TRACING"] = "true"
        
        # ë°ì´í„°ì…‹ ì´ˆê¸°í™”
        self._ensure_datasets()
    
    def _load_prompt_from_langsmith(self, prompt_name: str):
        """
        LangSmithì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ
        
        Args:
            prompt_name: í”„ë¡¬í”„íŠ¸ ì´ë¦„
            
        Returns:
            ChatPromptTemplate: ë¡œë“œëœ í”„ë¡¬í”„íŠ¸
        """
        try:
            # LangSmithì—ì„œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
            from langchain import hub
            prompt = hub.pull(prompt_name)
            return prompt
        except Exception as e:
            # hubì—ì„œ ì‹¤íŒ¨í•˜ë©´ ì§ì ‘ ê²€ìƒ‰ ì‹œë„
            try:
                prompts = list(self.langsmith_client.list_prompts())
                for p in prompts:
                    if hasattr(p, 'name') and p.name == prompt_name:
                        # í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ ChatPromptTemplateìœ¼ë¡œ ë³€í™˜
                        return ChatPromptTemplate.from_template(p.template)
                raise Exception(f"í”„ë¡¬í”„íŠ¸ '{prompt_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            except Exception as inner_e:
                raise Exception(f"LangSmithì—ì„œ í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {inner_e}")
    
    def _ensure_datasets(self):
        """LangSmith ë°ì´í„°ì…‹ ì¡´ì¬ í™•ì¸ ë° ìƒì„±"""
        datasets = [self.source_dataset, self.result_dataset, self.history_dataset]
        
        for dataset_name in datasets:
            try:
                existing_datasets = list(self.langsmith_client.list_datasets(dataset_name=dataset_name))
                if not existing_datasets:
                    self.langsmith_client.create_dataset(
                        dataset_name=dataset_name,
                        description=f"Agent QA ì‹œìŠ¤í…œ ë°ì´í„°ì…‹: {dataset_name}"
                    )
                    print(f"âœ… LangSmith ë°ì´í„°ì…‹ '{dataset_name}' ìƒì„± ì™„ë£Œ")
                else:
                    print(f"âœ… LangSmith ë°ì´í„°ì…‹ '{dataset_name}' í™•ì¸ ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ ë°ì´í„°ì…‹ '{dataset_name}' ì„¤ì • ì˜¤ë¥˜: {e}")

    def _resolve_run_url_from_handler(self, handler) -> Optional[str]:
        """RunCollectorì—ì„œ ìˆ˜ì§‘í•œ runë“¤ë¡œë¶€í„° ì ‘ê·¼ ê°€ëŠ¥í•œ ì¶”ì  URLì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        - ìš°ì„  name == 'RunnableSequence'ë¥¼ ì„ íƒ
        - run.parent_run_idë¥¼ ë”°ë¼ ë£¨íŠ¸ runê¹Œì§€ ì˜¬ë¼ê°„ í›„ run.url ì‚¬ìš©
        - run.urlì´ ì—†ìœ¼ë©´ projects ê²½ë¡œë¡œ í´ë°±
        """
        try:
            selected = None
            # ìš°ì„  RunnableSequence ìš°ì„  íƒìƒ‰
            for r in reversed(getattr(handler, "traced_runs", []) or []):
                if getattr(r, "name", "") == "RunnableSequence":
                    selected = r
                    break
            # ëŒ€ì•ˆ: ë§ˆì§€ë§‰ run ì‚¬ìš©
            if selected is None and getattr(handler, "traced_runs", None):
                selected = handler.traced_runs[-1]
            if not selected:
                return None
            run_id = getattr(selected, "id", None)
            if not run_id:
                return None
            # ë£¨íŠ¸ runê¹Œì§€ ìƒìŠ¹
            # runì´ ì €ì¥ë˜ê¸°ê¹Œì§€ ì•½ê°„ì˜ ì§€ì—°ì´ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¬ì‹œë„
            run = None
            for _ in range(10):
                try:
                    run = self.langsmith_client.read_run(run_id)
                    if run:
                        break
                except Exception:
                    pass
                time.sleep(0.3)
            if not run:
                return None
            # ë£¨íŠ¸ runê¹Œì§€ ìƒìŠ¹
            while getattr(run, "parent_run_id", None):
                run = self.langsmith_client.read_run(run.parent_run_id)
            # url í™•ë³´ ì¬ì‹œë„ (ë¹„ë™ê¸° ë°˜ì˜ ëŒ€ë¹„)
            for _ in range(10):
                url = getattr(run, "url", None)
                if url:
                    return url
                time.sleep(0.3)
            # urlì´ ë¹„ì–´ìˆë‹¤ë©´ í”„ë¡œì íŠ¸ ê¸°ë°˜ í´ë°±
            project = os.getenv("LANGSMITH_PROJECT", "llm-practice")
            return f"{self.base_web_url}/projects/{project}/runs/{run.id}"
        except Exception:
            return None
    
    @staticmethod
    def _sort_testcases_by_case_id(testcases: List[Dict]) -> List[Dict]:
        """case_idì— í¬í•¨ëœ ìˆ«ì ê¸°ì¤€ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬"""
        def key(tc: Dict) -> int:
            m = re.search(r"(\d+)", str(tc.get("case_id", "")))
            try:
                return int(m.group(1)) if m else 10**9
            except Exception:
                return 10**9
        return sorted(testcases, key=key)

    def _count_history_examples(self) -> int:
        try:
            return len(list(self.langsmith_client.list_examples(dataset_name=self.history_dataset)))
        except Exception:
            return 0
    
    def check_history_status(self):
        """íˆìŠ¤í† ë¦¬ ë°ì´í„°ì…‹ ìƒíƒœ í™•ì¸"""
        try:
            examples = list(self.langsmith_client.list_examples(dataset_name=self.history_dataset))
            print(f"\nğŸ“ˆ íˆìŠ¤í† ë¦¬ ë°ì´í„°ì…‹ ìƒíƒœ: {len(examples)}ê°œ ì¼€ì´ìŠ¤")
            
            for ex in examples:
                try:
                    case_id = ex.metadata.get("case_id") if ex.metadata else "Unknown"
                    scores = ex.outputs.get("scores", []) if ex.outputs else []
                    print(f"  - {case_id}: {len(scores)}íšŒ ì‹¤í–‰")
                except Exception:
                    pass
                
        except Exception:
            pass

    def backfill_history_from_results(self) -> int:
        """
        ê¸°ì¡´ ê²°ê³¼ ë°ì´í„°ì…‹ì—ì„œ íˆìŠ¤í† ë¦¬ ë°ì´í„°ì…‹ìœ¼ë¡œ ëˆ„ì  ë°±í•„
        Returns: ë°˜ì˜í•œ ì˜ˆì œ ìˆ˜
        """
        try:
            examples = list(self.langsmith_client.list_examples(dataset_name=self.result_dataset))
            applied = 0
            for ex in examples:
                case_id = ex.metadata.get("case_id") if ex.metadata else None
                question = ex.inputs.get("input") if ex.inputs else None
                outputs = ex.outputs or {}
                if not case_id or not question:
                    continue
                result = {
                    "case_id": case_id,
                    "question": question,
                    "answer": outputs.get("answer", ""),
                    "judge_accuracy_score": outputs.get("judge_accuracy_score", 0),
                    "reasoning": outputs.get("judge_reasoning", ""),
                }
                if self.save_result_to_history(result):
                    applied += 1
            if applied > 0:
                print(f"â™»ï¸ íˆìŠ¤í† ë¦¬ ë°±í•„ ì™„ë£Œ: {applied}ê°œ ê²°ê³¼ ë°˜ì˜")
            return applied
        except Exception as e:
            print(f"âš ï¸ íˆìŠ¤í† ë¦¬ ë°±í•„ ì¤‘ ì˜¤ë¥˜: {e}")
            return 0

    def load_testcases_from_excel(self, excel_path: str) -> List[Dict]:
        """
        TestCase.xlsxì—ì„œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ë¡œë“œ
        
        Args:
            excel_path: Excel íŒŒì¼ ê²½ë¡œ
            
        Returns:
            í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸ [{"case_id": str, "question": str}, ...]
        """
        try:
            # ì—¬ëŸ¬ ì‹œíŠ¸ë¥¼ ëª¨ë‘ ì½ì–´ ë³‘í•©
            all_sheets = pd.read_excel(excel_path, sheet_name=None)
            if isinstance(all_sheets, dict):
                frames = []
                for sheet_name, sdf in all_sheets.items():
                    if sdf is None or len(sdf) == 0:
                        continue
                    sdf["__sheet__"] = str(sheet_name)
                    frames.append(sdf)
                if not frames:
                    print("âš ï¸  ëª¨ë“  ì‹œíŠ¸ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
                    return []
                df = pd.concat(frames, ignore_index=True)
                print(f"ğŸ“Š Excel íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í–‰ | ì‹œíŠ¸ ìˆ˜: {len(all_sheets)}")
            else:
                df = all_sheets
                print(f"ğŸ“Š Excel íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(df)}ê°œ í–‰ | ì‹œíŠ¸ ìˆ˜: 1")
            print(f"ì»¬ëŸ¼: {list(df.columns)}")
            
            # ì»¬ëŸ¼ëª… ì •ê·œí™” (ëŒ€ì†Œë¬¸ì, ê³µë°± ì²˜ë¦¬)
            df.columns = df.columns.str.strip().str.lower()
            
            # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸
            required_columns = ['case_id', 'question', 'input']
            available_columns = df.columns.tolist()
            
            # ì»¬ëŸ¼ ë§¤í•‘ í™•ì¸ (ì •í™•ë§¤ì¹­ ìš°ì„ , ì—†ìœ¼ë©´ ìœ ì‚¬ ë§¤ì¹­)
            case_id_col = 'case_id' if 'case_id' in available_columns else None
            question_col = 'question' if 'question' in available_columns else None
            if question_col is None:
                for cand in ['input', 'query']:
                    if cand in available_columns:
                        question_col = cand
                        break
            if case_id_col is None:
                for col in available_columns:
                    if 'case' in col or 'id' in col:
                        case_id_col = col
                        break
            if question_col is None:
                for col in available_columns:
                    if 'question' in col or 'input' in col or 'query' in col:
                        question_col = col
                        break
            
            print(f"ğŸ” ê°ì§€ëœ ì»¬ëŸ¼ - case_id: {case_id_col}, question: {question_col}")
            
            if not case_id_col or not question_col:
                print(f"âš ï¸  í•„ìš”í•œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {available_columns}")
                # ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ ì»¬ëŸ¼ì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
                case_id_col = available_columns[0]
                question_col = available_columns[1] if len(available_columns) > 1 else available_columns[0]
                print(f"ğŸ’¡ ê¸°ë³¸ ì»¬ëŸ¼ ì‚¬ìš© - case_id: {case_id_col}, question: {question_col}")
            
            # ì™„ì „ ê³µë°± í–‰ ì œê±°
            df = df.dropna(how='all')

            testcases = []
            missing_case_id = 0
            missing_question = 0
            for _, row in df.iterrows():
                case_id = str(row.get(case_id_col, '')).strip()
                question = str(row.get(question_col, '')).strip()
                
                if not case_id or case_id.lower() == 'nan' or case_id == 'None':
                    missing_case_id += 1
                    continue
                if not question or question.lower() == 'nan' or question == 'None':
                    missing_question += 1
                    continue
                testcases.append({
                    "case_id": case_id,
                    "question": question
                })
            
            print(f"âœ… ìœ íš¨í•œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ {len(testcases)}ê°œ ë¡œë“œ ì™„ë£Œ")
            dropped = missing_case_id + missing_question
            if dropped > 0:
                print(f"â„¹ï¸  ì œì™¸ëœ í–‰ ìˆ˜: {dropped} (case_id ì—†ìŒ: {missing_case_id}, question ì—†ìŒ: {missing_question})")
            return testcases
            
        except Exception as e:
            print(f"âŒ Excel íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []
    
    def get_existing_case_ids(self) -> set:
        """
        LangSmith Agent_QA_Scenario ë°ì´í„°ì…‹ì—ì„œ ê¸°ì¡´ case_id ëª©ë¡ ì¡°íšŒ
        
        Returns:
            ê¸°ì¡´ case_id ì§‘í•©
        """
        try:
            examples = list(self.langsmith_client.list_examples(dataset_name=self.source_dataset))
            existing_case_ids = set()
            
            for example in examples:
                if example.metadata and "case_id" in example.metadata:
                    existing_case_ids.add(example.metadata["case_id"])
            
            print(f"ğŸ“‹ ê¸°ì¡´ ë°ì´í„°ì…‹ì—ì„œ {len(existing_case_ids)}ê°œ ì¼€ì´ìŠ¤ ë°œê²¬: {sorted(list(existing_case_ids))}")
            return existing_case_ids
            
        except Exception as e:
            print(f"âš ï¸  ê¸°ì¡´ ì¼€ì´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨ (ìƒˆ ë°ì´í„°ì…‹ì¼ ìˆ˜ ìˆìŒ): {e}")
            return set()
    
    def save_testcases_to_langsmith(self, testcases: List[Dict]) -> bool:
        """
        í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë¥¼ LangSmith Agent_QA_Scenario ë°ì´í„°ì…‹ì— ì €ì¥ (ì¤‘ë³µ ë°©ì§€)
        
        Args:
            testcases: í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ê¸°ì¡´ ì¼€ì´ìŠ¤ ID ì¡°íšŒ
            existing_case_ids = self.get_existing_case_ids()
            
            # ìƒˆë¡œìš´ ì¼€ì´ìŠ¤ë§Œ í•„í„°ë§
            new_testcases = [tc for tc in testcases if tc["case_id"] not in existing_case_ids]
            
            if not new_testcases:
                print(f"âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ê°€ ì´ë¯¸ ë°ì´í„°ì…‹ì— ì¡´ì¬í•©ë‹ˆë‹¤. (ì´ {len(testcases)}ê°œ)")
                return True
            
            print(f"\nğŸ“ {len(new_testcases)}ê°œ ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë¥¼ LangSmithì— ì €ì¥ ì¤‘...")
            print(f"   (ì „ì²´ {len(testcases)}ê°œ ì¤‘ {len(testcases) - len(new_testcases)}ê°œëŠ” ì´ë¯¸ ì¡´ì¬)")
            
            for i, tc in enumerate(new_testcases, 1):
                self.langsmith_client.create_example(
                    dataset_name=self.source_dataset,
                    inputs={"question": tc["question"]},
                    outputs=None,
                    metadata={
                        "case_id": tc["case_id"],
                        "source": "TestCase.xlsx"
                    }
                )
                print(f"  [{i}/{len(new_testcases)}] {tc['case_id']}: {tc['question'][:50]}...")
                time.sleep(0.1)  # API ë¶€í•˜ ë°©ì§€
            
            print(f"âœ… {len(new_testcases)}ê°œ ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ê°€ '{self.source_dataset}' ë°ì´í„°ì…‹ì— ì €ì¥ ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    def get_testcases_from_langsmith(self) -> List[Dict]:
        """
        LangSmith Agent_QA_Scenario ë°ì´í„°ì…‹ì—ì„œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì¡°íšŒ
        
        Returns:
            í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        """
        try:
            examples = list(self.langsmith_client.list_examples(dataset_name=self.source_dataset))
            testcases = []
            
            for example in examples:
                case_id = example.metadata.get("case_id") if example.metadata else str(example.id)
                question = example.inputs.get("question", "")
                
                if question:
                    testcases.append({
                        "case_id": case_id,
                        "question": question,
                        "example_id": str(example.id)
                    })
            
            print(f"ğŸ“‹ LangSmithì—ì„œ {len(testcases)}ê°œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì¡°íšŒ ì™„ë£Œ")
            
            # case_id ê¸°ì¤€ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
            testcases = self._sort_testcases_by_case_id(testcases)
            return testcases
            
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def generate_answer_with_gpt4o(self, question: str) -> str:
        """
        GPT-4oë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€ ìƒì„±
        
        Args:
            question: ì§ˆë¬¸
            
        Returns:
            GPT-4o ë‹µë³€
        """
        try:
            handler = RunCollectorCallbackHandler()
            response = self.gpt_model.invoke(question, config={"callbacks": [handler]})
            answer = response.content.strip()
            return answer
        except Exception as e:
            print(f"âŒ GPT-4o ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    
    def judge_answer_with_gpt4o(self, question: str, answer: str) -> Dict:
        """
        GPT-4o Judgeë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ í‰ê°€
        
        Args:
            question: ì§ˆë¬¸
            answer: ë‹µë³€
            
        Returns:
            {"score": int, "reasoning": str}
        """
        try:
            handler = RunCollectorCallbackHandler()
            judge_result = self.judge_chain.invoke({
                "question": question,
                "answer": answer
            }, config={"callbacks": [handler]})
            
            # JSON íŒŒì‹±
            judge_data = json.loads(judge_result)
            score = int(judge_data.get("score", 0))  # ì •ìˆ˜ë¡œ ë³€í™˜
            reasoning = judge_data.get("reasoning", "í‰ê°€ ì‹¤íŒ¨")
            
            # ì ìˆ˜ ë²”ìœ„ ê²€ì¦
            if not (0 <= score <= 5):
                print(f"âš ï¸  ì ìˆ˜ ë²”ìœ„ ì˜¤ë¥˜ ({score}), 0ìœ¼ë¡œ ì„¤ì •")
                score = 0
            
            # Judge RunnableSequence íŠ¸ë ˆì´ìŠ¤ URL ì¶”ì¶œ (ë£¨íŠ¸ run ê¸°ì¤€)
            trace_url = self._resolve_run_url_from_handler(handler)
            if not trace_url:
                # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: handler ë‚´ run_idë¥¼ ë¡œê·¸ë¡œ ë‚¨ê¸°ê³  ì €ì¥ì€ ìƒëµ
                if getattr(handler, "traced_runs", None):
                    try:
                        rid = getattr(handler.traced_runs[-1], "id", None)
                        if rid:
                            print(f"(debug) judge run_id: {rid}")
                    except Exception:
                        pass
            return {"score": score, "reasoning": reasoning, "trace_url": trace_url}
            
        except Exception as e:
            print(f"âŒ Judge í‰ê°€ ì‹¤íŒ¨: {e}")
            return {"score": 0, "reasoning": f"í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}
    
    
    def save_results_to_langsmith(self, results: List[Dict]) -> bool:
        """
        í‰ê°€ ê²°ê³¼ë¥¼ LangSmith ê²°ê³¼ ë°ì´í„°ì…‹ì— ì €ì¥
        
        Args:
            results: ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ [{"case_id", "question", "answer", "judge_accuracy_score", "reasoning"}, ...]
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            print(f"\nğŸ’¾ {len(results)}ê°œ í‰ê°€ ê²°ê³¼ë¥¼ LangSmithì— ì €ì¥ ì¤‘...")
            
            for i, result in enumerate(results, 1):
                # LangSmith ë°ì´í„° êµ¬ì¡°: inputì„ ë‹¨ì¼ í‚¤ë¡œ ì„¤ì •í•˜ì—¬ question ë‚´ìš©ì´ ì£¼ìš” í‘œì‹œë˜ë„ë¡ í•¨
                self.langsmith_client.create_example(
                    dataset_name=self.result_dataset,
                    inputs={
                        "input": result["question"]  # "input"ì´ë¼ëŠ” ë‹¨ì¼ í‚¤ ì‚¬ìš©
                    },
                    outputs={
                        "answer": result["answer"],
                        "judge_accuracy_score": result["judge_accuracy_score"],
                        "judge_reasoning": result.get("reasoning", ""),
                        **({"trace_url": result.get("trace_url")} if result.get("trace_url") else {})
                    },
                    metadata={
                        "case_id": result["case_id"],
                        "question": result["question"],
                        "judge_accuracy_score": result["judge_accuracy_score"],
                        "model_used": "gpt-4o",
                        "judge_model": "gpt-4o",
                        "evaluation_type": "judge_accuracy"
                    }
                )
                print(f"  [{i}/{len(results)}] {result['case_id']}: {result['judge_accuracy_score']}/5ì ")
                time.sleep(0.1)  # API ë¶€í•˜ ë°©ì§€
            
            print(f"âœ… ëª¨ë“  í‰ê°€ ê²°ê³¼ê°€ '{self.result_dataset}' ë°ì´í„°ì…‹ì— ì €ì¥ ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ í‰ê°€ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def save_single_result_to_langsmith(self, result: Dict) -> bool:
        """
        ë‹¨ì¼ í‰ê°€ ê²°ê³¼ë¥¼ LangSmithì— ì¦‰ì‹œ ì €ì¥
        """
        try:
            created = self.langsmith_client.create_example(
                dataset_name=self.result_dataset,
                inputs={
                    "input": result["question"]
                },
                outputs={
                    "answer": result["answer"],
                    "judge_accuracy_score": result["judge_accuracy_score"],
                    "judge_reasoning": result.get("reasoning", ""),
                    **({"trace_url": result.get("trace_url")} if result.get("trace_url") else {})
                },
                metadata={
                    "case_id": result["case_id"],
                    "question": result["question"],
                    "judge_accuracy_score": result["judge_accuracy_score"],
                    "model_used": "gpt-4o",
                    "judge_model": "gpt-4o",
                    "evaluation_type": "judge_accuracy"
                }
            )
            # ë°ì´í„°ì…‹ ë§í¬ ì¶œë ¥ ì œê±° (ìš”ì²­ì— ë”°ë¼)
            print(f"  - ì €ì¥ ì™„ë£Œ: {result['case_id']} ({result['judge_accuracy_score']}/5ì )")
            time.sleep(0.1)
            return True
        except Exception as e:
            print(f"  - ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def _get_base_web_url(self) -> str:
        """LangSmith ì›¹ URL ê¸°ë³¸ê°’ ê³„ì‚° (API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì›¹ ë„ë©”ì¸ìœ¼ë¡œ ì •ê·œí™”)"""
        import os
        endpoint = os.getenv("LANGSMITH_ENDPOINT", "https://api.smith.langchain.com")
        # ì›¹ ë„ë©”ì¸ìœ¼ë¡œ ì •ê·œí™”
        endpoint = endpoint.replace("https://api.", "https://").replace("/api", "")
        if not endpoint.startswith("http"):
            endpoint = "https://smith.langchain.com"
        return endpoint.rstrip("/")

    def save_result_to_history(self, result: Dict) -> bool:
        """
        ë™ì¼ case_id íˆìŠ¤í† ë¦¬ë¥¼ ë°°ì—´ë¡œ ëˆ„ì  ì €ì¥í•˜ëŠ” ì „ìš© ë°ì´í„°ì…‹ ê´€ë¦¬ í•¨ìˆ˜
        outputs í•„ë“œì— scores/answers/reasons/timestamps ë°°ì—´ì„ ìœ ì§€í•©ë‹ˆë‹¤.
        """
        try:
            # case_idì— í•´ë‹¹í•˜ëŠ” ê¸°ì¡´ ì˜ˆì œ íƒìƒ‰
            examples = list(self.langsmith_client.list_examples(dataset_name=self.history_dataset))
            target_example = None
            for ex in examples:
                if ex.metadata and ex.metadata.get("case_id") == result["case_id"]:
                    target_example = ex
                    break

            now_iso = datetime.now().isoformat()

            if target_example is None:
                # ìƒˆ ì˜ˆì œë¡œ ë°°ì—´ ì´ˆê¸°í™”
                created = self.langsmith_client.create_example(
                    dataset_name=self.history_dataset,
                    inputs={"input": result["question"]},
                    outputs={
                        "scores": [int(result["judge_accuracy_score"])],
                        "answers": [result["answer"]],
                        "reasons": [result.get("reasoning", "")],
                        "timestamps": [now_iso],
                        **({"trace_urls": [result.get("trace_url")] } if result.get("trace_url") else {}),
                    },
                    metadata={
                        "case_id": result["case_id"],
                        "model_used": "gpt-4o",
                        "judge_model": "gpt-4o",
                        "evaluation_type": "judge_accuracy",
                    },
                )
                # ë°ì´í„°ì…‹ ë§í¬ ì¶œë ¥ ì œê±° (ìš”ì²­ì— ë”°ë¼) - ì•„ë¬´ ê²ƒë„ ì¶œë ¥í•˜ì§€ ì•ŠìŒ
            else:
                # ê¸°ì¡´ ì˜ˆì œë¥¼ ë°°ì—´ appendë¡œ ì—…ë°ì´íŠ¸
                outs = target_example.outputs or {}
                outs["scores"] = list(outs.get("scores", [])) + [int(result["judge_accuracy_score"]) ]
                outs["answers"] = list(outs.get("answers", [])) + [ result["answer"] ]
                outs["reasons"] = list(outs.get("reasons", [])) + [ result.get("reasoning", "") ]
                outs["timestamps"] = list(outs.get("timestamps", [])) + [ now_iso ]
                if result.get("trace_url"):
                    outs["trace_urls"] = list(outs.get("trace_urls", [])) + [ result.get("trace_url") ]

                updated = self.langsmith_client.update_example(
                    example_id=str(target_example.id),
                    outputs=outs,
                )
                # ë°ì´í„°ì…‹ ë§í¬ ì¶œë ¥ ì œê±°

            time.sleep(0.1)
            return True
        except Exception as e:
            return False
    
    def run_full_evaluation(self, excel_path: str) -> bool:
        """
        ì „ì²´ í‰ê°€ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
        
        Args:
            excel_path: TestCase.xlsx íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        print("ğŸš€ ì‹¤ì œ Agent QA í‰ê°€ ì‹œìŠ¤í…œ ì‹œì‘")
        print("=" * 60)
        
        # 1. TestCase.xlsxì—ì„œ ë°ì´í„° ë¡œë“œ
        print("\n1ï¸âƒ£  TestCase.xlsxì—ì„œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ë¡œë“œ")
        testcases = self.load_testcases_from_excel(excel_path)
        if not testcases:
            print("âŒ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨")
            return False
        
        # 2. Agent_QA_Scenario ë°ì´í„°ì…‹ì— ì €ì¥
        print(f"\n2ï¸âƒ£  LangSmith '{self.source_dataset}' ë°ì´í„°ì…‹ì— ì €ì¥")
        if not self.save_testcases_to_langsmith(testcases):
            print("âŒ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨")
            return False
        
        # 3. ì €ì¥ëœ ë°ì´í„° ì¡°íšŒ
        print(f"\n3ï¸âƒ£  LangSmithì—ì„œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì¡°íšŒ")
        stored_testcases = self.get_testcases_from_langsmith()
        if not stored_testcases:
            print("âŒ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì¡°íšŒ ì‹¤íŒ¨")
            return False
        
        # 4. GPT-4oë¡œ ì§ˆì˜ ë° í‰ê°€
        print(f"\n4ï¸âƒ£  GPT-4oë¡œ ì§ˆì˜ ë° Judge í‰ê°€ ì‹¤í–‰")
        results = []
        
        for i, tc in enumerate(stored_testcases, 1):
            print(f"\n[{i}/{len(stored_testcases)}] ì²˜ë¦¬ ì¤‘: {tc['case_id']}")
            print(f"â“ ì§ˆë¬¸: {tc['question']}")
            
            # GPT-4oë¡œ ë‹µë³€ ìƒì„±
            answer = self.generate_answer_with_gpt4o(tc['question'])
            
            # GPT-4o Judgeë¡œ í‰ê°€
            judge_result = self.judge_answer_with_gpt4o(tc['question'], answer)
            
            # ê²°ê³¼ ì €ì¥
            result = {
                "case_id": tc['case_id'],
                "question": tc['question'],
                "answer": answer,
                "judge_accuracy_score": judge_result['score'],
                "reasoning": judge_result['reasoning'],
                "trace_url": judge_result.get('trace_url')
            }
            results.append(result)
            
            print(f"ğŸ’¡ ë‹µë³€: {answer[:100]}...")
            print(f"ğŸ“Š í‰ê°€ ìš”ì•½: {judge_result['score']}/5ì  - {judge_result['reasoning'][:100]}...")
            
            # API ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
            time.sleep(1)
        
        # 5. ê²°ê³¼ë¥¼ ê²°ê³¼ ë°ì´í„°ì…‹ì— ì €ì¥
        print(f"\n5ï¸âƒ£  LangSmith '{self.result_dataset}' ë°ì´í„°ì…‹ì— ê²°ê³¼ ì €ì¥")
        if not self.save_results_to_langsmith(results):
            print("âŒ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨")
            return False
        
        # 6. ìµœì¢… ìš”ì•½
        print(f"\nâœ… ì „ì²´ í‰ê°€ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
        print("=" * 60)
        print(f"ğŸ“Š ì²˜ë¦¬ í†µê³„:")
        print(f"  - ì´ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤: {len(results)}ê°œ")
        print(f"  - í‰ê·  ì ìˆ˜: {sum(r['judge_accuracy_score'] for r in results) / len(results):.2f}/5")
        print(f"  - ìµœê³  ì ìˆ˜: {max(r['judge_accuracy_score'] for r in results)}/5")
        print(f"  - ìµœì € ì ìˆ˜: {min(r['judge_accuracy_score'] for r in results)}/5")
        
        return True

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    excel_path = Path(__file__).parent / "TestCase.xlsx"
    
    if not excel_path.exists():
        print(f"âŒ TestCase.xlsx íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {excel_path}")
        return
    
    try:
        system = RealAgentQASystem()
        success = system.run_full_evaluation(str(excel_path))
        
        if success:
            print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print("ğŸ” LangSmithì—ì„œ ë‹¤ìŒ ë°ì´í„°ì…‹ì„ í™•ì¸í•˜ì„¸ìš”:")
            print(f"  - ì›ë³¸ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤: Agent_QA_Scenario")
            print(f"  - í‰ê°€ ê²°ê³¼: {system.result_dataset}")
        else:
            print("\nâŒ ì‘ì—… ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

def save_testcases_only(uploaded_excel_path: Optional[str] = None):
    """
    3. TestCase.xlsxë¥¼ Agent_QA_Scenario ë°ì´í„°ì…‹ì— ì €ì¥í•˜ëŠ” ê¸°ëŠ¥ë§Œ ì‹¤í–‰
    """
    # ìš°ì„ ìˆœìœ„: ì¸ì > í™˜ê²½ë³€ìˆ˜ > ê¸°ë³¸ íŒŒì¼
    env_path = os.getenv("UPLOADED_EXCEL_PATH")
    if uploaded_excel_path and os.path.exists(uploaded_excel_path):
        excel_path = Path(uploaded_excel_path)
        print(f"ğŸ“¤ ì—…ë¡œë“œëœ íŒŒì¼ ì‚¬ìš©: {excel_path.name}")
        print(f"   ê²½ë¡œ: {excel_path}")
    elif env_path and os.path.exists(env_path):
        excel_path = Path(env_path)
        print(f"ğŸ“¤ ì—…ë¡œë“œëœ íŒŒì¼ ì‚¬ìš©: {excel_path.name}")
        print(f"   ê²½ë¡œ: {excel_path}")
    else:
        excel_path = Path(__file__).parent / "TestCase.xlsx"
        print(f"ğŸ“ ê¸°ë³¸ íŒŒì¼ ì‚¬ìš©: TestCase.xlsx")
        print(f"   ê²½ë¡œ: {excel_path}")
    
    if not excel_path.exists():
        print(f"âŒ Excel íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {excel_path}")
        return
    
    try:
        print("ğŸ“¥ TestCase â†’ Agent_QA_Scenario ë°ì´í„°ì…‹ ì €ì¥ ì‹œì‘")
        print("="*60)
        
        system = RealAgentQASystem()
        
        # 1. Excelì—ì„œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ë¡œë“œ
        print(f"1ï¸âƒ£  Excel íŒŒì¼ì—ì„œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ë¡œë“œ: {excel_path.name}")
        testcases = system.load_testcases_from_excel(str(excel_path))
        
        if not testcases:
            print("âŒ ìœ íš¨í•œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 2. LangSmithì— ì €ì¥ (ì¤‘ë³µ ë°©ì§€)
        print("\n2ï¸âƒ£  LangSmith 'Agent_QA_Scenario' ë°ì´í„°ì…‹ì— ì €ì¥")
        success = system.save_testcases_to_langsmith(testcases)
        
        if success:
            print("\nâœ… í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì €ì¥ ì™„ë£Œ!")
            print("ğŸ” LangSmithì—ì„œ 'Agent_QA_Scenario' ë°ì´í„°ì…‹ì„ í™•ì¸í•˜ì„¸ìš”.")
        else:
            print("\nâŒ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")

def run_evaluation_only():
    """
    4. Agent_QA_Scenario ë°ì´í„°ì…‹ì—ì„œ GPT-4o í‰ê°€ë§Œ ì‹¤í–‰
    """
    try:
        print("ğŸš€ Agent_QA_Scenario â†’ GPT-4o í‰ê°€ ì‹¤í–‰ ì‹œì‘")
        print("="*60)
        
        system = RealAgentQASystem()
        
        # 1. LangSmithì—ì„œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì¡°íšŒ
        print("1ï¸âƒ£  LangSmithì—ì„œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì¡°íšŒ")
        testcases = system.get_testcases_from_langsmith()
        
        if not testcases:
            print("âŒ Agent_QA_Scenario ë°ì´í„°ì…‹ì—ì„œ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ’¡ ë¨¼ì € '3. TestCase.xlsx â†’ Agent_QA_Scenario ë°ì´í„°ì…‹ ì €ì¥'ì„ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
            return
        
        # í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ì •ë ¬: case_idì˜ ìˆ«ì ì˜¤ë¦„ì°¨ìˆœ (ì˜ˆ: TC_001, TC_002 ...)
        testcases = system._sort_testcases_by_case_id(testcases)
        
        print(f"ğŸ“‹ ì •ë ¬ëœ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤ ìˆœì„œ: {[tc['case_id'] for tc in testcases]}")
        
        
        # 2. ì „ê¸°ì°¨ RAG Agentë¥¼ í†µí•œ ì§ˆì˜ ë° Judge í‰ê°€ ì‹¤í–‰
        print(f"\n2ï¸âƒ£  ì „ê¸°ì°¨ RAG Agentë¡œ ì§ˆì˜ ë° Judge í‰ê°€ ì‹¤í–‰")
        results = []
        
        for i, tc in enumerate(testcases, 1):
            print(f"\n[{i}/{len(testcases)}] ì²˜ë¦¬ ì¤‘: {tc['case_id']}")
            print(f"â“ ì§ˆë¬¸: {tc['question']}")
            
            # ì „ê¸°ì°¨ RAG Agentë¡œ ë‹µë³€ ìƒì„±
            try:
                from ev_rag_agent import get_ev_agent
                agent = get_ev_agent()
                answer, _ = agent.answer(tc["question"]) 
            except Exception as e:
                print(f"RAG Agent ì˜¤ë¥˜ë¡œ GPT-4o ì§ì ‘ ë‹µë³€ìœ¼ë¡œ í´ë°±: {e}")
                answer = system.generate_answer_with_gpt4o(tc["question"]) 
            
            # Judgeë¡œ í‰ê°€
            judge_result = system.judge_answer_with_gpt4o(tc["question"], answer)
            
            # ìš”ì•½ ì¶œë ¥ (ë‹µë³€ ë¨¼ì €, í‰ê°€ ìš”ì•½ ë‚˜ì¤‘ì—)
            print(f"ğŸ’¡ ë‹µë³€: {answer[:100]}...")
            print(f"âš–ï¸ í‰ê°€ ìš”ì•½: {judge_result['score']}/5ì  - {judge_result.get('reasoning', '')[:100]}...")
            
            
            single_result = {
                "case_id": tc["case_id"],
                "question": tc["question"],
                "answer": answer,
                "judge_accuracy_score": judge_result["score"],
                "reasoning": judge_result["reasoning"],
                "trace_url": judge_result.get("trace_url")
            }
            
            # ì¦‰ì‹œ ì €ì¥
            print("ğŸ’¾ ê²°ê³¼ ì¦‰ì‹œ ì €ì¥ ì¤‘...")
            system.save_single_result_to_langsmith(single_result)
            # íˆìŠ¤í† ë¦¬ì—ë„ ëˆ„ì 
            system.save_result_to_history(single_result)
            results.append(single_result)
            
            # API ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
            time.sleep(1)
        
        # 3. ì²˜ë¦¬ í†µê³„ ìš”ì•½
        success = len(results) > 0
        # íˆìŠ¤í† ë¦¬ ë°ì´í„°ì…‹ì´ ë¹„ì–´ìˆë‹¤ë©´ ê¸°ì¡´ ê²°ê³¼ë¥¼ ë°±í•„í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
        if self := system:  # ëª…ì‹œì  ì°¸ì¡° íšŒí”¼ìš©
            try:
                if system._count_history_examples() == 0:
                    system.backfill_history_from_results()
            except Exception:
                pass
        
        if success:
            print("\nâœ… í‰ê°€ ì™„ë£Œ!")
            print("="*60)
            print(f"ğŸ“Š ì²˜ë¦¬ í†µê³„:")
            print(f"  - ì´ í…ŒìŠ¤íŠ¸ì¼€ì´ìŠ¤: {len(results)}ê°œ")
            print(f"  - í‰ê·  ì ìˆ˜: {sum(r['judge_accuracy_score'] for r in results) / len(results):.2f}/5")
            print(f"  - ìµœê³  ì ìˆ˜: {max(r['judge_accuracy_score'] for r in results)}/5")
            print(f"  - ìµœì € ì ìˆ˜: {min(r['judge_accuracy_score'] for r in results)}/5")
            print("\nğŸ‰ GPT-4o í‰ê°€ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print("ğŸ” LangSmithì—ì„œ ë‹¤ìŒ ë°ì´í„°ì…‹ì„ í™•ì¸í•˜ì„¸ìš”:")
            print(f"  - í‰ê°€ ê²°ê³¼: {system.result_dataset}")
        else:
            print("\nâŒ í‰ê°€ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨")
            
    except Exception as e:
        print(f"âŒ í‰ê°€ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")

# OpenEvals ì‹¤í–‰ ë¡œì§ ì œê±°ë¨ (ë©”ë‰´ì—ì„œ ì‚­ì œ)

if __name__ == "__main__":
    main()
